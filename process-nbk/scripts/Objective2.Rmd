---
title: |
  ![](../data/Signet_INF_3.jpg){width=400px style="display: block; margin:0 auto"}
  <center>Twitter Analysis</center>
  ![](../data/logo.svg){width=100px style="display: block; margin:0 auto"}
author: Vishnu Jayanand
date: "June 19th, 2021"
bibliography: references.bib
output:
  html_document:
    css: style.css
    df_print: paged
    toc: true
    number_sections : true
    toc_depth: 2
    toc_float: true 
    theme: flatly
    highlight: textmate
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center} \includegraphics[width=5in,height=5in]{Signet_INF_3.jpg}\LARGE\\}
- \posttitle{\end{center}} \usepackage{fancyhdr} \pagestyle{fancy} 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# **Overview** {-} 

We propose to know the impact of COVID-19 tackling infodemics and misinformation on
Twitter. This is done by extracting recent popular tweets from a specific location across different countries.
It will help us describe the false information that is spread with the sole purpose of causing confusion and
harm. We target to extract hashtags like #covid19, #misinformation, #fakenews, #disinformation, #, etc.,
to get the related posts about it and analyze how the information processing and decision-making behaviors
are compromised. We perform sentimental analysis on the tweets to understand the sentiments of people
which is crucial during the time of this pandemic

# Twitter Data

We have primarily two datasets - one of them contains tweets from the onset of the pandemic and the other are very recent tweets (June 2021). Our main objective here is to figure out 
how the sentiments have changed over the months.

For the security purposes, we show the skeletal code to extract the tweets using fake credentials. We would load the data via .rds file for our extracted tweets. [@twitterTutorial]

```{r connection, eval=FALSE}
library(rtweet)
library(dplyr)
library(tidyr)
library(twitteR)
library(tidytext)

appname <- "CovidDistress"
key <- "ogRXvxribQAEt9tJKQ1rEd0c0"
secret <- "HlvVRoFg73JJcpcGjYxUWBagWratEIrdagPCeaiToWTKa15vCO"
access_token <- "15914217-8YYyRRAxRBL0Vu9Y0tAjVFfPvdJdYByfmsiVpLEoD"
access_secret <- "oeXIkYHBTQpGRxZCKI4q67UN3L8PuJfwb0su6EOkIk22f" 

twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret,
  access_token = access_token,
  access_secret = access_secret,
  set_renv = TRUE)

corona_tweets <- search_tweets(q = "#covid19 OR #coronavirus", n=20000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)

saveRDS(corona_tweets, "../data/tweets2021.rds")
```

We can now load saved RDS file using the command below
```{r loadRDS}
tweets2021_raw <- readRDS("../data/tweets2021.rds")

```

There are `r nrow(tweets2021_raw)` tweets from the dataset which is more than what we intended. This is because we set **retryonratelimit** to TRUE. These tweets are dated from ***`r format(as.Date(min(tweets2021_raw$created_at)), "%B %d %Y")`*** to ***`r format(as.Date(max(tweets2021_raw$created_at)), "%B %d, %Y")`***

Here's a sample row from the dataset
```{r echo=FALSE}
gt::gt(tweets2021_raw[1,])
```

We also have few other datasets that has tweets from 2020 and with other hashtags

```{r eval=FALSE}

tweets2021_vaccine<- search_tweets(q = "#vaccine", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
tweets2021_vaccine_and_covid19<- search_tweets(q = "#covid19 AND #vaccine", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
tweets2021_job <- search_tweets(q = "#job", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
tweets2021_job_covid19 <- search_tweets(q = "#covid19 AND #job", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
tweets2021_jobloss <- search_tweets(q = "#covid19 AND #jobloss", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
tweets2021_donate <- search_tweets(q = "#covid19 AND #donate", n=10000, include_rts=FALSE, lang="en", retryonratelimit = TRUE)
```


**To be added**
1. English Word Cloud (Old Vs New)
2. Frequency Chart (Old Vs New)
3. Postive and Negative Common Words (Old Vs New)
4. Sentiment Analysis Bar graph (Old Vs New)
5. A world map for seeing the tweets world wide (Old Vs New If Possible)
6. German Word Cloud
7. Sentiment Regarding Vaccine Word Cloud/Bar Graph
8. Prefferred Vaccine Word Cloud/Bar Graph
9. Mental Health Word Cloud/Bar Graph

# Data Cleaning and Preparation

To explore the data and extract insights in the most efficient way, we decided to clean up the data. We use only the relevant columns

```{r datacleaning, echo=FALSE, message=FALSE}
colnames(tweets2021_raw)
```
For more powerful insights, we use only the columns "text", "hashtags" and "location" and we speciafically clean up the columns text and hashtags. Let's do some basic analysis to see the top locations of tweets.

```{r locations, echo=FALSE, background=TRUE}
library(tidyverse)
```

```{r, message=FALSE}

tweets2021_raw %>% 
  filter(!is.na(location) & location != "") %>% 
  count(location, sort = TRUE) %>% 
  top_n(10)

```


It is however important to note that Twitter API is based on relevance and not completedness
https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview

```{r}


```



# **References** 