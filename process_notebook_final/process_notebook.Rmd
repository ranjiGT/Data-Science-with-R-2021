---
title: |
  <center>R Process Notebook</center>
   ![](./data/images/covid.png){width=650px style="display: block; margin:0 auto"}  
bibliography: references.bib
output:
  html_document:
    df_print: paged
    toc: true
    number_sections : true
    toc_depth: 6
    toc_float: true 
    theme: readable
    highlight: breezedark
---
# **Analysis & Predictions of the Impact of COVID-19 ** {-} 

**TEAM MEMBERS:** ![](./data/DataSciR.jpeg){width=180px height=180px align=right}

 * Madhuri Sajith
 * Usama Ashfaq
 * Vishnu Jayanand
 * Sujith Nyarakkad Sudhakaran
 * Ranjiraj Rajendran NAir
 

# **Overview and Motivation**
The coronavirus COVID-19 pandemic is an unprecedented health crisis that has impacted the world to a large extent. According to WHO, mental disorders are one of the leading causes of disability worldwide, so considering that this pandemic has caused further complications to mental ailment. The stress, anxiety, depression stemming from fear, isolation, and stigma around COVID-19 affected all of us in one way or another. We could see that many people are losing their jobs and the elderly are isolated from their usual support network. The issue is that the effects of the pandemic and mental health, maybe for longer-lasting than the disease itself.    

In this limelight, although the measures are taken to slow the spread of the virus, it has affected our physical activity levels, our eating behaviors, our sleep patterns, and our relationship with addictive substances, including social media. Into this last point, both our increased use of social media while stuck at home, as well as the increased exposure to disaster news past year, have amplified the negative effects of social media on our mental health. This motivates us to perform some diagnostic analysis of this pattern and portray some meaningful insights on global survey data.

# **Related Work**

Though COVID-19 pandemic was started in early 2019, the cpnsequences and the after effects has continued to affect the people mentally and physically. 

There are plethora of research papers that extensively work on corona dataset across diffrent countries and analyses the sentiments of people who lives there. Vijay et al. [@Vijay] made analysis on twets in India during 2019-20 and calculated sentiment scores of the people during this timeline. They observed how the sentiment score was getting better with increasing time. Sethi et al.[@Goran] presented a paper in analysing the tweets across six different countries like USA, UK, Spain, Italy, Sweden and Germany. The analysis is majorly focussed on to understand what positive sentiment is shared between people across these six countries. In our observation, we analyse and compare the sentiments of people in the year 2020 and 2021 not by sentiment score but by using the positive and nagative words they used in their tweets.

# **Initial Questions**
USing our analysis, we try to answer the following three questions by formulating three major objectives <br>


### Based on the tweets and hashtags, how has the sentiments of people changed from the year 2020 to 2021?






# **Data**

### Objective 1 Dataset

### Twitter Data 
We formulated two datasets for analysisng and comparing the tweets in the year 2020 and 2021.
    . The dataset that was obtained by retrieving tweets by using`twitteR` which provides an interface and access to Twitter web API
    . Covid-tweets_2020.csv which contains tweets which are consolidated across months in 2020
    
We have performed preprocessing for the tweets as the tweets contained several special characters, numbers and spaces.

### Objective 3 Dataset

```{r medical_data_before_preprocess, echo=FALSE, }
med <- read.csv("./data/2020-05-27_WOLFRAM_PATIENT_MEDICAL_DATASET_NOVEL_CORONAVIRUS.csv")
head(med,10)
```

```{r setup, include=FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
source("scripts/package_installer.R")
```


```{r init_medical_data, message=FALSE, warning=FALSE, include=TRUE}
# preprocessing script for medical dataset
source("scripts/medical_data_preprocess.R")
```



### Twitter Data
We will work on the most recent dataset aggregated from Twitter using tritteR and rtweet libraries within a particular timeframe.Here twitteR which provides an interface and access to Twitter web API, rtweet which acts as the client for Twitter's REST and stream APIs will be used to retrieve data.

 
Data Cleaning was done by removing white Spaces, links, punctuation, stop words, retweets, emotions, mentions, control characters, digits and converting to lower case.

```{r preprocessing_twitter, echo=FALSE, eval=FALSE}
#Remove retweets
corona.tweets <- strip_retweets(corona.tweets,strip_manual = T, strip_mt = T)
#Grabbing text data from tweets
corona.text <- sapply(corona.tweets, function(x) x$getText())
#Clean text data to remove emoticons and other symbols
corona.text <- iconv(corona.text,'UTF-8','ASCII')
#Clean text data to remove twitter mentions
corona.text <- gsub("@[[:alpha:]]*","",corona.text)
#Clean text data to remove blank spaces
corona.text <- gsub("[:blank:]", "", corona.text)
#Clean text data to remove control characters
corona.text <-gsub("[:cntrl:]", "", corona.text)
#Clean text data to remove digits
corona.text <-gsub("[[:digit:]]", "", corona.text)
#Clean text data to remove special characters and other unwanted 
corona.text <-gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ",  corona.text)
corona.text <-gsub("@\\w+", "", corona.text)
corona.text <-gsub("http\\w+", "", corona.text)
corona.corpus <- Corpus(VectorSource(corona.text))
#Term document frequency - remove stop words and converting to lower case.
term.doc.matrix <- TermDocumentMatrix(corona.corpus,
                                      control = list(removePunctuation=T,
                                                     stopwords =c('https','http',stopwords('english')),
                                                     removeNumbers = T,
                                                     tolower = T))
```


# **Exploratory Data Analysis**

We performed exploratory data analysis on all the components of the datasets. Below are the sections of EDA:

* Analysis on Patient Medical Data
* Predictions using Time Series Data
* Analysis on Twitter Data

## **Analysis on Patient Medical Data** 

### **All Patients by Age**  
Plotting all the Patients Age Distribution on Histogram
```{r medical_data_age_graph, fig.align='center'}
ggplotly(ggplot(med, aes(Age, fill=factor(Sex)))+geom_histogram(binwidth = 5,color = 'black')+ theme_bw() 
               + scale_color_brewer(palette = "Dark2")
               + scale_fill_brewer(palette = "Dark2"))
```
We compared the age distribution among the patients ( Child, Adult and Senior Adult) . The age group between 40 to 60 have been affected the most. From our analysis ans well as confirmed by International research the percentage of children among the confirmed COVID-19 patients is low, ranging from 1% in young children to 6% in older children.  Children with COVID-19 do have the same symptoms as adults. The most common symptoms in children are coughing, fever and sore throat. Worldwide, very few children with COVID-19 have died.In clusters of patients, adults are almost always the source patient. On basis of this, decisions such as re-opening of schools and childcare facilities were made [@Children].


### *Chronic Diseases w.r.t Gender* 
Plot the distribution of Male and Female of any age with Chronic Diseases
```{r}
true.chron <- subset(med, med$ChronicDiseaseQ=='True')
ggplotly(ggplot(true.chron, aes(Age, fill = factor(Sex)))+ geom_histogram(binwidth = 10, color = 'black')+ facet_wrap(~Sex) + theme_bw() )
```

Everyone exposed to virus are at risk. However, some people are more vulnerable than others to become severely ill and more in need for medical facilities. According to "Centers for Disease Control and Prevention" , people of any age with certain chronic diseases are at higher risk for severe illness from covid-19. 

### *Frequency of Chronic Diseases* 
Plot to see the most common chronic diseases 

```{r medical_data_chronic_desease_freq_graph, fig.align='center'}
write.table(med$ChronicDiseases, "./data/char.txt", append = FALSE, sep = ",", row.names = F,col.names=F)
sentences <- scan("./data/char.txt","character",sep="\n", quiet = TRUE)
sentences <- gsub("[^A-Za-z,]+"," ",sentences)
words <- strsplit(sentences,",")
words.freq <- table(unlist(words))
dm <- data.frame(cbind(names(words.freq),as.integer(words.freq)))
dm$X2 <- as.numeric(as.character(dm$X2))
dm <- dm[order(dm$X2, decreasing = F),]
ggplotly(ggplot(dm, aes(x = reorder(X1,X2), y = X2)) +
           geom_col(fill = '#7104FF', color = 'black' ) +
           labs(title="Chronic Diseases ",
                x = NULL,
                y = "Frequency") +
           coord_flip() + theme_bw())
```

When a person having chronic medical condition gets infected with coronavirus, they are likely to face an increased risk of developing severe symptoms. We try to find recovery to death ratio for patients with and without COVID-19. The ratio is 17:49 for patients with chronic disease and 53:8 for patients without chronic disease. This shows that people without chronic medical conditions have higher chances of survival than those who have it.  

### *Recovery and Death Ratio w.r.t Chronic Diseases*
```{r recovery_to_death_ratio, warning=FALSE, eval=FALSE}
library(dplyr)
recoveryDeathRatio <- function(x){ 
  if (!is.na(x)){
    return (1)
  }
  else { return(0)}
}
  
# recovery:death for patients with chronic disease
chronic <- subset(med, med$ChronicDiseaseQ=='True'  & (!is.na(med$DateOfDeath) | !is.na(med$DateOfDischarge)))
chronic
#chronic <- chronic[,c("ChronicDiseaseQ","DateOfDischarge","DateOfDeath")]

chronic$DateOfDischarge <- sapply(chronic$DateOfDischarge, recoveryDeathRatio)
chronic$DateOfDeath <- sapply(chronic$DateOfDeath, recoveryDeathRatio)

print(MASS::fractions(sum(chronic$DateOfDischarge)/sum(chronic$DateOfDeath)))

# recovery:death for patients without chronic disease 
not.chronic <- subset(med, med$ChronicDiseaseQ=='False'  & (!is.na(med$DateOfDeath) | !is.na(med$DateOfDischarge)))
#not.chronic <- not.chronic[,c("ChronicDiseaseQ","DateOfDischarge","DateOfDeath")]

not.chronic$DateOfDischarge <- sapply(not.chronic$DateOfDischarge, recoveryDeathRatio)
not.chronic$DateOfDeath <- sapply(not.chronic$DateOfDeath, recoveryDeathRatio)

print(MASS::fractions(sum(not.chronic$DateOfDischarge)/ sum(not.chronic$DateOfDeath)))
```


### **Word Cloud of Common Symptoms** 
Wordcloud for showing the overall most common symptoms
```{r medical_data_symptom_wordcloud,fig.height=6, fig.width=8, fig.align='center'}
com.symp <- subset(med, !is.na(med$Symptoms))
com.symp <- com.symp[,"Symptoms"]
write.table(com.symp, "./data/symp.txt", append = FALSE, sep = ",", row.names = F,col.names=F)
sent <- scan("./data/symp.txt","character",sep="\n", quiet = TRUE)
sent <- gsub("[^A-Za-z,]+"," ",sent)
symp.words <- strsplit(sent,",")
word.count <- table(unlist(symp.words))
doc <- data.frame(cbind(names(word.count),as.integer(word.count)))
doc <- doc[c(-1,-34),]
names(doc) <- c('symptoms','freq')
doc$freq <- as.numeric(as.character(doc$freq))
doc <- doc[order(doc$freq, decreasing = T),]
wordcloud2(doc, size = 2 , color = 'random-light', backgroundColor = 'black')
```
From our wordcloud we can say that cough and fever are most common symptoms among the patients. People generally develop signs and symptoms, including mild respiratory
symptoms and fever, on an average of 5-6 days after infection (mean incubation period 5-6
days, range 1-14 days). 



### **Distribution of Symptoms in Wuhan, China** 
Plotting pie chart to display the common symptoms seen initially in Wuhan, China which is the origin.
```{r medical_data_symptom_piechart, warning=FALSE, fig.align='center', message=FALSE}
df <- data.frame(med$Country,med$LivesInWuhan,med$Symptoms)
df1 <- subset(df, (!is.na(med.Country) & !is.na(med.LivesInWuhan) & !is.na(med.Symptoms))) #remove NA values from these 3 columns
chinaOnly <- df1[df1$med.Country == 'China' & df1$med.LivesInWuhan == 'True' ,] #Retreive only china and Only Wuhan since that is the 
colnames(chinaOnly)[colnames(chinaOnly) == 'med.Symptoms'] <- 'Symptoms'
library(plyr)

dt <- count(chinaOnly, 'Symptoms') #Data frame with frequencies of each symtoms

dt$rank <- rank(-dt$freq,ties.method="min") #Get the ranking
df <- dt[order(dt$rank,decreasing = F),]

topsym_inWuhan <- head(df,8) 
dat <- topsym_inWuhan[,c('Symptoms', 'freq')]
fig <- plot_ly(dat, labels = ~ Symptoms, values = ~freq, type = 'pie')
fig <- fig %>% layout(title = 'Most Common Symptoms in Wuhan',
                      xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
                      yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
fig

```

Based on this, awareness was created for the rest of the world during the initial stages reagrding these symptoms. It describes the highest ranked symptoms seen together in the origin of COVID-19 that is Wuhan,China.Covid-19 symptoms are  non-specific and the disease presentation can range from no symptoms(asymptomatic) to severe pneumonia and death. 

### **Patient Locations** 
Plotting of locations of patients on world map
```{r medica_data_patient_locations,out.width="100%", message=FALSE, warning=FALSE}
library(stringr)
library(sf)
library(reshape2)
library(plotly)

LatLon <- colsplit(gsub("([A-Z])([0-9])", "\\1\\.\\2", med$GeoPosition), 
                   ",", c("Long", "Lat"))

new <- data.frame(LatLon, med$Country)

plot_geo(new) %>%
  add_markers(
    x = ~Lat, y = ~Long, hoverinfo = "text",
    text = ~paste(new$med.Country)) %>% layout (title = 'Patient Locations across the World')

```
This shows us the locations of patients of our across the globe plotted on the world map.The outbreak spread from the Chinese city of Wuhan to more than 180 countries and territories affecting every continent except Antartica. Efforts to stamp out the pneumonia-like illness have driven all the countries to enforce lockdowns, widespread halts of international travel, mass layoffs and shattered financial markets.


### **Analysis using t-tests and Box-plots**
Plotting a box plot to see the average age group likely to recover
```{r medica_data_agerecover_boxplot}
alive <- subset(med, (!is.na(DischargedQ) | !is.na(DateOfDischarge)))
alive <- alive[,c("Age","Sex","DischargedQ","DateOfDischarge")]
ggplotly(ggplot(data=alive, aes(" ", Age)) + geom_boxplot(color='black', fill = '#10AB05') +
  stat_summary(fun=mean, colour="darkred", geom="point", shape=18, size=3,show.legend  = FALSE) +
  theme_bw()+
  ggtitle("Analyzing age group who are Likely to Recover "))
```


Plotting a box plot to see the average age group unlikely to recover
```{r medica_data_deceasesbyage_boxplot}
dead <- subset(med, !is.na(med$Age) & !is.na(med$Sex) & !is.na(med$DeathQ))
dead <- dead[,c("Age","Sex","DeathQ")]
ggplotly(ggplot(data=dead, aes(" ", Age)) + geom_boxplot(color='black', fill = '#3BA6E5') +
  stat_summary(fun=mean, colour="darkred", geom="point", shape=18, size=3,show.legend  = FALSE) +
  theme_bw() +
  ggtitle("Analysis Age group who are Unlikely to Recover"))

```

Performing t-tests to see how confident we are that older people are more likely to die than younger people from COVID-19.
```{r medica_data_age_ttest}
t.test(dead$Age, alive$Age, alternative="two.sided", conf.level = 0.95)
```

Now here, we first make a separate data frame consisting of only age, gender and death columns. From this , we create a boxplot to  analyse the age of people who have died and of those who did not. The rhombus in the boxplot shows the mean(y) value of those who survived as 43.9 , while the mean(x) for those who have died as 65.5. So, this indeed shows that older people have lower chances of survival. 

But is this true universally for the population? How confident are we that this is true? So, we perform T-tests to guage our confidence and to see if we can trust the means we got. In this case, we will use a 95% confidence interval.

Looking at the confidence interval, we can say with 95% certainity, that the age difference between between patients who have died and those who have not is from 18.7 to 24.5 years. Now, if we look at the p-value. It is almost 0. This means that there is ~0% chance that we obtained such extreme result randomly from this sample under the null hypothesis (which is that the ages of the two groups are equal). For this reason, we can reasonably reject the null hypothesis (under the conventional significance level of 0.05) and say that people who have died from COVID-19 are indeed older than those who did not. 
Articles which says that men indeed do have a higher coronavirus death rate[@cnn].




### **Timeline of Patients** 
Covid-19 is a confusing illness , wrapped with uncertainty. There has not been sufficient scientific studies to tell precisely how long it takes for a person to recover. While some possible ranges have been identified. These seems to vary from person to person. According to WHO, recovery times tend to be about 2 weeks for those with mild disease and about 3-2 weeks for those with severe/critical disease.

#### **Recovered Patients** 

Plot to show the time taken for recovery by each patients
```{r medical_data_RecoveryDuration_plot, warning=FALSE, fig.align='center'}
library(timelineS)
time.df <- subset(med, !is.na(med$Age)  & !is.na(med$DateOfOnsetSymptoms)
                  & (!is.na(med$DateOfDeath) | !is.na(med$DateOfDischarge)))
time.df <- time.df[,c("Age","DateOfOnsetSymptoms","DateOfDeath","DateOfDischarge")]
discharge.days <- durCalc(time.df, start="DateOfOnsetSymptoms", end="DateOfDischarge",timeunit = 'days')
discharge.days <- discharge.days[,c("Age","days")]
ggplotly(ggplot(discharge.days, aes(days, Age))+ geom_point(aes(color=Age),size = 2) + scale_color_gradientn(colors = c("#00AFBB", "#E7B800", "#FC4E07"))+ theme_bw() +ggtitle("Days between Onset Symptoms and Discharge"))

```


#### **Deceased Patients** 
Plot to show the time taken for recovery by each patients
```{r}

  #mean time (in days) for death
  death.days <-durCalc(time.df, start="DateOfOnsetSymptoms", end="DateOfDeath",timeunit = 'days')
  death.days <- death.days[,c("Age","days")]
  ggplotly(ggplot(death.days, aes(days, Age))+ geom_point(aes(color=Age),size = 2) +
                    scale_color_gradientn(colors = c("#00AFBB", "#E7B800", "#FC4E07")) +
                    theme_bw() +
                    ggtitle("Days between date of onset of symptoms and date of death"))
```


## **Predictions using Time series Data**

We have chosen the ARIMA model to predict the cases of next 'n' days because the current situation is so uncertain, for eg., number of cases in Italy suddenly rose and it even beat China which was the origin, similarly lot of cases are increasing in India right now. So from this we can say that we should be ready to expect the unexpected. ARIMA models ceases any non-seasonal time series  changes that is exhibited and gives a good prediction when compared to others which we used like Naive forecasting model that gave us similar results everyday since it only considers the last recent value and forecasts the current one. There is no adjustments made in case of any changes due to extrernal factors. We faced the same issue with Holt-Winters forecasting model as well.

To get an idea about how the virus could spread across a country in the next couple of days, we decided to create a predictor to forecast those values. The idea behind this was that, based on the current predictions, a country could take appropriate actions to cut down the spread of the virus like enforing a lockdown if the number of cases are increasing ih huge numbers or lift up the lockdown if things are getting better.

The name of a Country is taken as input, along with the type of cases to predict (confirmed, recovered, dead, active) and the number of days to predict and this generates a set of predictions based on the given input. 

```{r predictor_init, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(ggplot2)
library(reshape2)
library(lubridate)
library(readr)
library(forecast)
library(fpp2)
library(TTR)
library(dplyr)
library(RCurl)
library(plotly)
```

We pull the current John Hopkins dataset from their server and create required datasets from it
```{r predictor_collect_data, cache=TRUE}
confirmed_url = "https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv&filename=time_series_covid19_confirmed_global.csv"
recovered_url = "https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_recovered_global.csv&filename=time_series_covid19_recovered_global.csv"
deaths_url = "https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_deaths_global.csv&filename=time_series_covid19_deaths_global.csv"
john_confirmed <- read.csv(textConnection(getURL(confirmed_url)))
john_recovered <- read.csv(textConnection(getURL(recovered_url)))
john_deaths <- read.csv(textConnection(getURL(deaths_url)))
```

Global parameters used throughout the predictor code :
We enter a country country for which the predictions need to be made, set the number of days to predict and also choose which category of cases to predict (confirmed, recovered, dead, active)

```{r predictor_globals}
country = "Germany"
days_to_predict = 7
type_to_predict <- "confirmed"
```

Filtering out data from the country selected as input.
Generating the active cases dataset using the other three datasets (active_cases = confirmed - recovered - dead)
then generating one data frame with all cases of a particular country

```{r predictor_data_selection}
jc <- john_confirmed %>% filter(Country.Region == country) %>% dplyr::select(c(-1,-2,-3,-4)) %>% colSums()
jr <- john_recovered %>% filter(Country.Region == country) %>% dplyr::select(c(-1,-2,-3,-4)) %>% colSums()
jd <- john_deaths %>% filter(Country.Region == country) %>% dplyr::select(c(-1,-2,-3,-4)) %>% colSums()
ja <- jc-jr-jd
john_selected <- as.data.frame(t(rbind(jc,jr,jd,ja)))
colnames(john_selected) <- c("confirmed", "recovered", "dead", "active")
```

### Generating the Time series Data
Generating a sequence of dates from first to last which is used to create the time series data.

```{r predictor_timeseries_generation}
year = colnames(john_confirmed)
last_date = tail(year, n=1)
last_date <- mdy(gsub("X","",last_date))
last_date <- last_date + lubridate::days(days_to_predict)
dates <- seq(as.Date("2020-01-22"), as.Date(last_date), by = "day")
data_ts <- ts(john_selected[type_to_predict], start = c(2020, as.numeric(format(dates[1], "%j"))), frequency = 365)
head(data_ts)
```

### Arima Model for Forecasting
The time series data is fed into arima model to generate a prediction for the next n days mentioned in input.
```{r predictor_arima_forecasting}
arima_model <- auto.arima(data_ts)
forecast_arima = forecast::forecast(arima_model, h=days_to_predict)
forecast_arima = as.data.frame(as.data.frame(forecast_arima)$`Point Forecast`)
```

### Binding
Then binding them using rbind to form one single dataset.
```{r predictor_plotting_preprocess}
names(forecast_arima)[1] <- "cases"
forecast_arima$cases <- round(forecast_arima$cases, 0)
data_ts <- as.data.frame(cbind(data_ts,label = "actual"))
forecast_arima <- as.data.frame(cbind(forecast_arima, label = "predicted"))
names(data_ts)[1] <- "cases"
names(forecast_arima)[1] <- "cases"
data_and_forecast <- rbind(data_ts, forecast_arima)
# attaching dates to this dataset which will be used as x axis during plotting
data_and_forecast <- cbind(data_and_forecast, dates)
data_and_forecast$cases <-as.integer(data_and_forecast$cases)
```

### **Prediction of COVID-19 Cases**
Plotting the time series of the actual cases followed by the predicted ones using line graph.
```{r predictor_plotting, fig.align= 'center'}
fig <- ggplot(data_and_forecast)+ 
      geom_line(aes(x = dates, y = cases, color =label))+
      geom_point(aes(x = dates, y = cases, color =label))+
      scale_color_manual(values=c("blue", "red")) +
      theme_minimal()
ggplotly(fig)
```

### Summary of Predicted Values

```{r predictor_summary}
tail(data_and_forecast[,c(3,1,2)], 5+days_to_predict)
```

Furthermore we developed a Shiny App found here 

Using the Shiny App *https://mohammed-shaikh.shinyapps.io/Covid/* , we can predict the different type of cases (confirmed, recovered, dead, active) for different countries between 1 to 15 days.

Based on [@bund] and [@mdr] we have identified points in time which are important steps towards the containment of the virus in Germany as well as in other countries of the world.  
For a better overview, we have specialized in the following countries: India, Germany, Italy, Brazil, US and Spain. <br>
The following table shows the events and dates presented in the interactive timeline.  
Based on these dates we have created plots for confirmed,recovered and deaths, which include the countries listed above.

 


```{r init_timeline, echo=FALSE}
library(timevis)
```

```{r timeline_ceation, eval=FALSE}
data_covid <- data.frame(
  id      = 1:25,
  content = c("China officially reports cases to WHO",
              "In France, the first evidence of the virus",
              "The virus has reached Germany",
              "The WHO declares a 'health emergency of international concern'",
              "France reports the first death in Europe",
              "Coronavirus infections have now been confirmed for the first time in Baden-Württemberg and North Rhine-Westphalia",
              "There are infections in about 60 countries. According to the WHO, there are around 3,000 deaths.",
              "The WHO declares a pandemic",
              "In most of the federal states schools and day care centres are already closed, others will follow. Also border controls and entry bans",
              "Italy is now the country with the most officially reported deaths worldwide",
              "Germany: Federal and state governments agree on strict exit and contact restrictions",
              "At over 140,000, more infections are now known in the USA than have been officially recorded in any other country in the world",
              "The nationwide contact restrictions are extended until 19 April.",
              "Start of the OVGU lecture period - digital",
              "Germany: The contact restrictions are extended until 3 May.",
              "Germany: first relaxation of corona protection measures",
              "Germany: In all federal states the mouth protection obligation applies",
              "Germany: further relaxation of corona restrictions",
              "Germany: end of controls at the German external borders (gradually)",
              "Brazil reports 15,000 new infections within 24 hours",
              "Germany: The number of new infections is below 1,000 (R) for the tenth consecutive day.",
              "The number of infections registered worldwide exceeds five million.",
              "Germany: economic stimulus package adopted",
              "China reports highest increase in new infections since April",
              "Germany: The government has launched the Corona Warning App"),
  start   = c("2019-12-31", "2020-01-24", "2020-01-27", "2020-01-30", "2020-02-15", "2020-02-24", "2020-03-02", 
              "2020-03-11", "2020-03-16", "2020-03-19", "2020-03-22", "2020-03-29", "2020-04-01", "2020-04-06",
              "2020-04-15", "2020-04-20", "2020-04-27", "2020-05-06", "2020-05-13", "2020-05-17", "2020-05-19",
              "2020-05-21", "2020-06-04", "2020-06-14", "2020-06-16"),
  end     = c(NA          ,           NA,           NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA,    NA)
)
timevis(data_covid)
```

```{r, echo=FALSE, fig.align='center'}
htmltools::tags$iframe(title = "My embedded document", src = "./data/timeline_export.html", width="800", height ="700" )
```


```{r timeline_plots_init, echo=FALSE}
library(tidyr)
library(ggplot2)
library(readr)
library(dplyr)
library(data.table)
```

```{r timeline_plots_data_collection}
john_deaths <- read.csv("./data/time_series_covid19_deaths_global_24_06_20_mod.csv")
john_recovered <- read.csv("./data/time_series_covid19_recovered_global_24_06_20_mod.csv")
john_confirmed <- read.csv("./data/time_series_covid19_confirmed_global_24_06_20_mod.csv")

countries = c("India", "Germany", "Italy", "Brazil", "US", "Spain")
dates <- c("2020-01-24", "2020-01-27", "2020-01-30", "2020-02-15", "2020-02-24", "2020-03-02", 
              "2020-03-11", "2020-03-16", "2020-03-19", "2020-03-22", "2020-03-29", "2020-04-01", "2020-04-06",
              "2020-04-15", "2020-04-20", "2020-04-27", "2020-05-06", "2020-05-13", "2020-05-17", "2020-05-19",
              "2020-05-21", "2020-06-04", "2020-06-14", "2020-06-16")
selector <- letters[seq( from = 1, to = 24 )]
```

```{r timeline_plot_confirmed, fig.align='center'}
library(dplyr)
## only confirmed ##

jc1 <- john_confirmed %>% filter(Country.Region == countries[1]) %>% select(all_of(selector))
jc2 <- john_confirmed %>% filter(Country.Region == countries[2]) %>% select(all_of(selector))
jc3 <- john_confirmed %>% filter(Country.Region == countries[3]) %>% select(all_of(selector))
jc4 <- john_confirmed %>% filter(Country.Region == countries[4]) %>% select(all_of(selector))
jc5 <- john_confirmed %>% filter(Country.Region == countries[5]) %>% select(all_of(selector))
jc6 <- john_confirmed %>% filter(Country.Region == countries[6]) %>% select(all_of(selector))
john_selected_confirmed <- t(rbind(jc1,jc2,jc3,jc4,jc5,jc6))
colnames(john_selected_confirmed) <- countries
confirmed.df <- as.data.frame(t(john_selected_confirmed))
confirmed.df$country <- countries

# Line plots with dots - confirmed
ggplot(confirmed.df, aes(x = country)) + 
 geom_line(aes(y=a,group=2, color = dates[1]), size = 0.5) +
  geom_line(aes(y=b,group=2, color = "2020-01-27"), size = 0.5) +
 geom_line(aes(y=c,group=2, color = "2020-01-30"), size = 0.5) +
 geom_line(aes(y=d,group=2, color = "2020-02-15"), size = 0.5) +
 geom_line(aes(y=e,group=2, color = "2020-02-24"), size = 0.5) +
 geom_line(aes(y=f,group=2, color = "2020-03-02"), size = 0.5) +
 geom_line(aes(y=g,group=2, color = "2020-03-11"), size = 0.5) +
  geom_line(aes(y=h,group=2, color = "2020-03-16"), size = 0.5) +
  geom_line(aes(y=i,group=2, color = "2020-03-19"), size = 0.5) +
  geom_line(aes(y=j,group=2, color = "2020-03-22"), size = 0.5) +
  geom_line(aes(y=k,group=2, color = "2020-03-29"), size = 0.5) +
  geom_line(aes(y=l,group=2, color = "2020-04-01"), size = 0.5) +
  geom_line(aes(y=m,group=2, color = "2020-04-06"), size = 0.5) +
  geom_line(aes(y=n,group=2, color = "2020-04-15"), size = 0.5) +
  geom_line(aes(y=o,group=2, color = "2020-04-20"), size = 0.5) +
  geom_line(aes(y=p,group=2, color = "2020-04-27"), size = 0.5) +
  geom_line(aes(y=q,group=2, color = "2020-05-06"), size = 0.5) +
  geom_line(aes(y=r,group=2, color = "2020-05-13"), size = 0.5) +
  geom_line(aes(y=s,group=2, color = "2020-05-17"), size = 0.5) +
  geom_line(aes(y=t,group=2, color = "2020-05-19"), size = 0.5) +
  geom_line(aes(y=u,group=2, color = "2020-05-21"), size = 0.5) +
  geom_line(aes(y=v,group=2, color = "2020-06-04"), size = 0.5) +
  geom_line(aes(y=w,group=2, color = "2020-06-14"), size = 0.5) +
  geom_line(aes(y=x,group=2, color = "2020-06-16"), size = 0.5) +

  geom_point(aes(y=a))+
  geom_point(aes(y=b))+
  geom_point(aes(y=c))+
  geom_point(aes(y=d))+
  geom_point(aes(y=e))+
  geom_point(aes(y=f))+
  geom_point(aes(y=g))+
  geom_point(aes(y=h))+
  geom_point(aes(y=i))+
  geom_point(aes(y=j))+
  geom_point(aes(y=k))+
  geom_point(aes(y=l))+
  geom_point(aes(y=m))+
  geom_point(aes(y=n))+
  geom_point(aes(y=o))+
  geom_point(aes(y=p))+
  geom_point(aes(y=q))+
  geom_point(aes(y=r))+
  geom_point(aes(y=s))+
  geom_point(aes(y=t))+
  geom_point(aes(y=u))+
  geom_point(aes(y=v))+
  geom_point(aes(y=w))+
  geom_point(aes(y=x))+

  labs(x="country", y="confirmed", color = "Legend") +
  coord_flip() +
  labs(title="Number of confirmed cases - in the space of time from 2020-01-24 to 2020-06-16", 
           subtitle="based on Johns Hopkins data set and the timline") +  
  theme(
    legend.position = c(.95, .80),
    legend.justification = c("right", "top") )

```


```{r timeline_plot_recovered, fig.align='center'}
## only recovered ##
jr1 <- john_recovered %>% filter(Country.Region == countries[1]) %>% select(all_of(selector))
jr2 <- john_recovered %>% filter(Country.Region == countries[2]) %>% select(all_of(selector))
jr3 <- john_recovered %>% filter(Country.Region == countries[3]) %>% select(all_of(selector))
jr4 <- john_recovered %>% filter(Country.Region == countries[4]) %>% select(all_of(selector))
jr5 <- john_recovered %>% filter(Country.Region == countries[5]) %>% select(all_of(selector))
jr6 <- john_recovered %>% filter(Country.Region == countries[6]) %>% select(all_of(selector))
john_selected_recovered <- t(rbind(jr1,jr2,jr3,jr4,jr5,jr6))
colnames(john_selected_recovered) <- countries
recovered.df <- as.data.frame(t(john_selected_recovered))
recovered.df$country <- countries

# Line plots with dots - recovered
ggplot(recovered.df, aes(x = country)) + 
  geom_line(aes(y=a,group=2, color = "2020-01-24"), size = 0.5) +
  geom_line(aes(y=b,group=2, color = "2020-01-27"), size = 0.5) +
  geom_line(aes(y=c,group=2, color = "2020-01-30"), size = 0.5) +
  geom_line(aes(y=d,group=2, color = "2020-02-15"), size = 0.5) +
  geom_line(aes(y=e,group=2, color = "2020-02-24"), size = 0.5) +
  geom_line(aes(y=f,group=2, color = "2020-03-02"), size = 0.5) +
  geom_line(aes(y=g,group=2, color = "2020-03-11"), size = 0.5) +
  geom_line(aes(y=h,group=2, color = "2020-03-16"), size = 0.5) +
  geom_line(aes(y=i,group=2, color = "2020-03-19"), size = 0.5) +
  geom_line(aes(y=j,group=2, color = "2020-03-22"), size = 0.5) +
  geom_line(aes(y=k,group=2, color = "2020-03-29"), size = 0.5) +
  geom_line(aes(y=l,group=2, color = "2020-04-01"), size = 0.5) +
  geom_line(aes(y=m,group=2, color = "2020-04-06"), size = 0.5) +
  geom_line(aes(y=n,group=2, color = "2020-04-15"), size = 0.5) +
  geom_line(aes(y=o,group=2, color = "2020-04-20"), size = 0.5) +
  geom_line(aes(y=p,group=2, color = "2020-04-27"), size = 0.5) +
  geom_line(aes(y=q,group=2, color = "2020-05-06"), size = 0.5) +
  geom_line(aes(y=r,group=2, color = "2020-05-13"), size = 0.5) +
  geom_line(aes(y=s,group=2, color = "2020-05-17"), size = 0.5) +
  geom_line(aes(y=t,group=2, color = "2020-05-19"), size = 0.5) +
  geom_line(aes(y=u,group=2, color = "2020-05-21"), size = 0.5) +
  geom_line(aes(y=v,group=2, color = "2020-06-04"), size = 0.5) +
  geom_line(aes(y=w,group=2, color = "2020-06-14"), size = 0.5) +
  geom_line(aes(y=x,group=2, color = "2020-06-16"), size = 0.5) +
  
  
  geom_point(aes(y=a))+
  geom_point(aes(y=b))+
  geom_point(aes(y=c))+
  geom_point(aes(y=d))+
  geom_point(aes(y=e))+
  geom_point(aes(y=f))+ 
  geom_point(aes(y=g))+ 
  geom_point(aes(y=h))+ 
  geom_point(aes(y=i))+ 
  geom_point(aes(y=j))+ 
  geom_point(aes(y=k))+ 
  geom_point(aes(y=l))+ 
  geom_point(aes(y=m))+ 
  geom_point(aes(y=n))+ 
  geom_point(aes(y=o))+ 
  geom_point(aes(y=p))+ 
  geom_point(aes(y=q))+ 
  geom_point(aes(y=r))+ 
  geom_point(aes(y=s))+ 
  geom_point(aes(y=t))+ 
  geom_point(aes(y=u))+ 
  geom_point(aes(y=v))+ 
  geom_point(aes(y=w))+ 
  geom_point(aes(y=x))+
  
  labs(x="country", y="recovered", color = "Legend") +
  coord_flip() +
  labs(title="Number of recovered cases - in the space of time from 2020-01-24 to 2020-06-16", 
       subtitle="based on Johns Hopkins data set and the timline") +  
  # theme(legend.position="top")
  theme(
    legend.position = c(.95, .80),
    legend.justification = c("right", "top") )

```

```{r timeline_plot_deaths, fig.align='center'}
## only deaths ##

jd1 <- john_deaths %>% filter(Country.Region == countries[1]) %>% select(all_of(selector))
jd2 <- john_deaths %>% filter(Country.Region == countries[2]) %>% select(all_of(selector))
jd3 <- john_deaths %>% filter(Country.Region == countries[3]) %>% select(all_of(selector))
jd4 <- john_deaths %>% filter(Country.Region == countries[4]) %>% select(all_of(selector))
jd5 <- john_deaths %>% filter(Country.Region == countries[5]) %>% select(all_of(selector))
jd6 <- john_deaths %>% filter(Country.Region == countries[6]) %>% select(all_of(selector))
john_selected_deaths <- t(rbind(jd1,jd2,jd3,jd4,jd5,jd6))
colnames(john_selected_deaths) <- countries
deaths.df <- as.data.frame(t(john_selected_deaths))
deaths.df$country <- countries

# Line plots with dots - deaths
ggplot(deaths.df, aes(x = country)) + 
  geom_line(aes(y=a,group=2, color = "2020-01-24"), size = 0.5) +
  geom_line(aes(y=b,group=2, color = "2020-01-27"), size = 0.5) +
  geom_line(aes(y=c,group=2, color = "2020-01-30"), size = 0.5) +
  geom_line(aes(y=d,group=2, color = "2020-02-15"), size = 0.5) +
  geom_line(aes(y=e,group=2, color = "2020-02-24"), size = 0.5) +
  geom_line(aes(y=f,group=2, color = "2020-03-02"), size = 0.5) +
  geom_line(aes(y=g,group=2, color = "2020-03-11"), size = 0.5) +
  geom_line(aes(y=h,group=2, color = "2020-03-16"), size = 0.5) +
  geom_line(aes(y=i,group=2, color = "2020-03-19"), size = 0.5) +
  geom_line(aes(y=j,group=2, color = "2020-03-22"), size = 0.5) +
  geom_line(aes(y=k,group=2, color = "2020-03-29"), size = 0.5) +
  geom_line(aes(y=l,group=2, color = "2020-04-01"), size = 0.5) +
  geom_line(aes(y=m,group=2, color = "2020-04-06"), size = 0.5) +
  geom_line(aes(y=n,group=2, color = "2020-04-15"), size = 0.5) +
  geom_line(aes(y=o,group=2, color = "2020-04-20"), size = 0.5) +
  geom_line(aes(y=p,group=2, color = "2020-04-27"), size = 0.5) +
  geom_line(aes(y=q,group=2, color = "2020-05-06"), size = 0.5) +
  geom_line(aes(y=r,group=2, color = "2020-05-13"), size = 0.5) +
  geom_line(aes(y=s,group=2, color = "2020-05-17"), size = 0.5) +
  geom_line(aes(y=t,group=2, color = "2020-05-19"), size = 0.5) +
  geom_line(aes(y=u,group=2, color = "2020-05-21"), size = 0.5) +
  geom_line(aes(y=v,group=2, color = "2020-06-04"), size = 0.5) +
  geom_line(aes(y=w,group=2, color = "2020-06-14"), size = 0.5) +
  geom_line(aes(y=x,group=2, color = "2020-06-16"), size = 0.5) +
  
  
  geom_point(aes(y=a))+
  geom_point(aes(y=b))+
  geom_point(aes(y=c))+
  geom_point(aes(y=d))+
  geom_point(aes(y=e))+
  geom_point(aes(y=f))+ 
  geom_point(aes(y=g))+ 
  geom_point(aes(y=h))+ 
  geom_point(aes(y=i))+ 
  geom_point(aes(y=j))+ 
  geom_point(aes(y=k))+ 
  geom_point(aes(y=l))+ 
  geom_point(aes(y=m))+ 
  geom_point(aes(y=n))+ 
  geom_point(aes(y=o))+ 
  geom_point(aes(y=p))+ 
  geom_point(aes(y=q))+ 
  geom_point(aes(y=r))+ 
  geom_point(aes(y=s))+ 
  geom_point(aes(y=t))+ 
  geom_point(aes(y=u))+ 
  geom_point(aes(y=v))+ 
  geom_point(aes(y=w))+ 
  geom_point(aes(y=x))+
  
  labs(x="country", y="deaths", color = "Legend") +
  coord_flip() +
  labs(title="Number of deaths - in the space of time from 2020-01-24 to 2020-06-16", 
       subtitle="based on Johns Hopkins data set and the timline") +  
  theme(
    legend.position = c(.95, .80),
    legend.justification = c("right", "top") )

```


## **Twitter Analysis**
### Visualize Words Contributed to Positive and Negative Sentiments
Here multiple graphs are plotted based on the tweet texts to identify the sentiment orientation and to analyze most frequent words that have been used to express particular emotions. For plotting the graphs, 10000 tweets are retrieved via TWitter Search API between 2020-03-14 and 2020-06-30.

```{r twitter_init, echo=FALSE, warning=FALSE, message=FALSE}
library(twitteR)
library(plyr)
library(dplyr)
library(tm)
library(ggplot2)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
library(SentimentAnalysis)
library(syuzhet)
library(wordcloud2)
library(reshape2)
library(scales)
library(lubridate)
library(webshot)
library(htmlwidgets)
```

Connect to twitter search API using twitteR
```{r twitter-credential, warning=FALSE, echo=FALSE}

#Keys to connect Twitter
consumer_key <- "dgnpiFq8gKSxUveX1lPCBIMmy"
consumer_secret <- "9YXiSffRXWfVNXTNJ7sUMUjjL6VzEBcjSUxWmcg0R6CsKZLcCB"
access_token <- "1269576425734049792-xOzOusJmjLcEOlfUZR3S0aPnN6ljE4"
access_secret <- "4vcMn8CL519261P5Xlq5RsMpD3QwSLO82AjbP1H8zH67e"
```

```{r connect-to-twitter, warning=FALSE, message=FALSE, cache=TRUE}
#Connect to Twitter
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

#Search tweets
tweet.list <- c("corona","covid19","pandemic","virus", "covid", "corona virus","covid19pandemic","stay home", "stay safe")
corona.tweets <- searchTwitter(tweet.list ,lang = 'en', n=10000, since = '2020-03-14', until ='2020-06-30' )
corona.tweets <- strip_retweets(corona.tweets,strip_manual = T, strip_mt = T)

```

Here tweets are categorized as positive or negative and then what are the words that have contributed most for their sentiment. From this chart, we can analyze, what are the words people have used frequently to express their positive or negative feelings.
```{r top-words-contributed-to-sentiment, warning=FALSE, fig.align='center'}

#Grabbing text data from tweets
corona.text <- sapply(corona.tweets, function(x) x$getText())

#Clean text data - remove emoticons and other symbols
corona.text <- iconv(corona.text,'UTF-8','ASCII')

#Remove twitter mentions
corona.text <- gsub("@[[:alpha:]]*","", corona.text)

# Removing blank spaces, punctuation, links, extra spaces, special characters and other unwanted things.
corona.text = gsub("[:blank:]", "", corona.text)
corona.text = gsub("[[:punct:]]", "", corona.text)
corona.text = gsub("[:cntrl:]", "", corona.text)
corona.text = gsub("[[:digit:]]", "", corona.text)
corona.text = gsub("[:blank:]", "", corona.text)
corona.text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ",  corona.text)
corona.text = gsub("@\\w+", "", corona.text)
corona.text = gsub("http\\w+", "", corona.text)


corona.corpus <- Corpus(VectorSource(corona.text))

doc.term.matrix <- DocumentTermMatrix(corona.corpus,control = list(removePunctuation=T,
                                                                   stopwords = c("corona","covid19","pandemic","virus", "covid", "corona virus","covid19pandemic","stay home", "stay safe",'http','https',stopwords('en')),
                                                                   removeNumbers = T,
                                                                   tolower = T))
#Plot the graph
corona.doc.term.matrix <- tidy(doc.term.matrix)
ap_sentiments <- corona.doc.term.matrix %>%
    inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments %>%
  dplyr::count(sentiment, term, wt = count) %>%
  filter(n >= 1) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot2::ggplot(aes(term, n, fill = sentiment)) +
  scale_fill_discrete(name = "Sentiment", labels = c("Negative", "Positive")) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Words") +
  ylab("Contribution to Sentiment")

```

### Bar Plot for Emotion and Frequency

```{r nrc-analysis, warning=FALSE}
#Get nrc emotions
sentiment <- get_nrc_sentiment(corona.text)

#Bar plot of emotion and count
barplot(colSums(sentiment), 2,col = rainbow(10),ylab = 'Count',main = 'Sentiment Scores for Corona')
```

Here tweets are categorized and analyzed for eight emotions "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust", "negative", "positive" using NRC sentiment dictionary and bar graph is plotted for each sentiment. This chart will give us a low level detail description about the emotions that people had during the pandemic period.

### Word Cloud of NRC Emotion
```{r word-cloud-by-NRC-emotion}
#Get words based on the emotion
wordcloud_tweet = c(
  paste(corona.text[sentiment$anger > 0], collapse=" "),
  paste(corona.text[sentiment$anticipation > 0], collapse=" "),
  paste(corona.text[sentiment$disgust > 0], collapse=" "),
  paste(corona.text[sentiment$fear > 0], collapse=" "),
  paste(corona.text[sentiment$joy > 0], collapse=" "),
  paste(corona.text[sentiment$sadness > 0], collapse=" "),
  paste(corona.text[sentiment$surprise > 0], collapse=" "),
  paste(corona.text[sentiment$trust > 0], collapse=" ")
)


#Create corpus
corpus = Corpus(VectorSource(wordcloud_tweet))

#Create document term matrix
tdm = TermDocumentMatrix(corpus)

#Convert as matrix
tdm = as.matrix(tdm)
tdmnew <- tdm[nchar(rownames(tdm)) < 11,]

#Column name binding
colnames(tdm) = c('Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust')
colnames(tdmnew) <- colnames(tdm)
comparison.cloud(tdmnew, random.order=FALSE,
                 colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"),
                 title.size=1, max.words=300, scale=c(0.8, 0.3), rot.per=0.4)
```
Here a word cloud is created for the eight emotions "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust" derived using NRC sentiment dictionary.

### Timeline of tweets
```{r sentiments-with-time, warning=FALSE}

corona_df = ldply(corona.tweets, function(t) t$toDataFrame())

#Check when tweet the most
corona_df$date <- day(corona_df$created)
corona_df$hour <- hour(corona_df$created)

#Cleaning
#Remove twitter mentions
corona_df$text <- gsub("@[[:alpha:]]*","", corona_df$text)
#Remove URLs
corona_df$text = gsub("http[^[:space:]]*", "",corona_df$text)


#Remove retweet entities 
corona_df$text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)"," ",corona_df$text)

#Remove quotes
corona_df$text = gsub("'s|'s|[...]", "", corona_df$text)

#Remove at people 
corona_df$text = gsub("@\\w+", " ", corona_df$text)
#Remove punctuation 
corona_df$text = gsub("[[:punct:]]", " ", corona_df$text)

#Remove single letters.
corona_df$text = gsub(" *\\b[[:alpha:]]{1}\\b *", "", corona_df$text)

#Remove unnecessary spaces
corona_df$text = gsub("[ \t]{2,}", " ", corona_df$text)

#Remove leading and trailing whitespaces 
corona_df$text = gsub("^\\s+|\\s+$", "", corona_df$text)

#Convert text object to corpus object to be recognized by tm
text_corpus <- Corpus(VectorSource(corona_df$text))
#Convert text to lower case
text_corpus <- tm_map(text_corpus, tolower)
#Remove words that we searched
text_corpus <- tm_map(text_corpus, removeWords, c("corona","covid19","pandemic","virus", "covid19", "corona virus","covid19pandemic","stay home", "stay safe",'http','https'))
#Remove english stop words
text_corpus <- tm_map(text_corpus, removeWords, stopwords("english"))
#Remove punctuation
text_corpus <- tm_map(text_corpus, removePunctuation)
#Remove whitespaces
text_corpus = tm_map(text_corpus, stripWhitespace)
#Stem words in the corpus 
text_corpus<-tm_map(text_corpus, stemDocument)

#Cleaned corpus to data frame
text_df <- data.frame(text_clean = get("content", text_corpus), stringsAsFactors = FALSE)
#Bind data frame with orginal
corona_df <- cbind.data.frame(corona_df, text_df)
#Sentiment analysis
corona_sentiment <- analyzeSentiment(corona_df$text_clean)
corona_sentiment <- dplyr::select(corona_sentiment, 
                                  SentimentGI, SentimentHE,
                                  SentimentLM, SentimentQDAP, 
                                  WordCount)
corona_sentiment <- dplyr::mutate(corona_sentiment, 
                                  mean_sentiment = rowMeans(corona_sentiment[,-5]))
corona_sentiment <- dplyr::select(corona_sentiment, 
                                  WordCount, 
                                  mean_sentiment)

corona_df <- cbind.data.frame(corona_df, corona_sentiment)
full_sentiment <- dplyr::mutate(corona_df, sentiment_score = if_else(mean_sentiment < 0, -1, 1) )
full_sentiment <- dplyr::mutate(full_sentiment, sentiment = if_else(mean_sentiment < 0, "Negative", "Positive") )
full_sentiment <- dplyr::filter(full_sentiment, sentiment == "Negative" | sentiment == "Positive")

ggplot(full_sentiment, aes(x = hour , fill=sentiment)) + 
  ggtitle("Hourly Positive and Negative Tweet Counts") + 
  geom_line(aes(color = sentiment), size = 2, stat = 'count') +
  scale_fill_discrete(name = "Sentiment", labels = c("Negative", "Positive")) +
  xlab("Hour") + 
  ylab("Tweet Count") + 
  ylim(c(300, 1200)) + 
  scale_color_manual(values = c("#00AFBB", "#E7B800")) 
```
Here the timeline  will show how the positive and negative tweets have changed with time. Here the chart is created for tweets on 29th June 2020 during 21:00 and 23:00 hour,and it shows how the sentiment counts have varied with the time.



### Analysis of German Tweets Word Cloud
```{r german-twitter-credentials, warning=FALSE, echo=FALSE}


consumer_key <- "MVK9iV1VgUSZJgyJ8e8wqiUM2"
consumer_secret <- "mXVLzU2LtA9lX1qgcGuM50aSh4wf3R6s6RwpaP2gytvOZ6T8UE"
access_token <- "1269576425734049792-gymp1PWf411qGTXbnRuUFuUHspx9kp"
access_secret <- "FmOyrjwPwuIlYEEX4NU3wea4zIyyXHFcbdUkGgK6W25eG"
```
Here a word cloud is created by retrieving German tweets. This will give us a idea, what words are used frequently in Germany during the crisis situation.

```{r german-word-cloud, warning=FALSE, message=FALSE, cache=TRUE}
library(twitteR)
#Connect to twitter
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets <- c("#corona","#covid19","#pandemic","#virus")

#Returning tweets
corona.tweets <- searchTwitter(tweets,lang='de',n=10000,since = "2020-03-14",until = "2020-06-30")
corona.tweets <- strip_retweets(corona.tweets)

#Grabbing text data from tweets
corona.text <- sapply(corona.tweets, function(x) x$getText())

#Fetching retweets
r_pattern <- grep("(RT|via)((?:\\b\\W*@\\w+)+)",corona.text,ignore.case = T)
retweets <- corona.text[r_pattern]

#Clean text data - remove emoticons and other symbols
corona.text <- iconv(corona.text,'UTF-8','ASCII')
corona.corpus <- Corpus(VectorSource(corona.text))

#Term document frequency
term.doc.matrix <- TermDocumentMatrix(corona.corpus,
                                      control = list(removePunctuation=T,
                                                      stopwords = c("corona","covid19","pandemic","virus", "covid", "corona virus","covid19pandemic","stay home", "stay safe",'http','https',stopwords('german')),
                                                     removeNumbers = T,
                                                     tolower = T))

#Convert object into matrix 
term.doc.matrix <- as.matrix(term.doc.matrix)

#Get word counts
word.freq <- sort(rowSums(term.doc.matrix),decreasing = T)
dm <- data.frame(word = names(word.freq), freq = word.freq)

#Create the word cloud for German
set.seed(123)
wordcloud(dm$word, dm$freq,min.freq = 2,
          max.words=600, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"),scale=c(1.0,0.90))
```


Here, the same German word cloud is plotted in a different theme, which is more user friendly and easy to identify the frequent words.
```{r german-word-cloud-2}
#Create word cloud 2 for German
cloud2_de <- wordcloud2(data=dm, size=1.6, color='random-dark',shape = 'pentagon')
saveWidget(cloud2_de, "tmp2.html", selfcontained = F)
webshot("tmp2.html", "wc2_de.png", delay = 5)
```


# **Final analysis**

### **Patient Medical Data**
Our overall goal was to exploit the sample patient dataset. Each observation of the dataset is related to the details of the confirmed COVID-19 patient such as age, DatOfOnsetSymptoms , DateOfDischarge etc.
Among 15466 observations, 6006 (38.83%)are Females and 9460 (61.17%) are Males. 766(4.95%) are 18 and below, 12059(77.97%) are between 20 and 60 inclusive, 2641(17.07%) above 60. The low count of children suggests that there is a relatively low attack rate in this age group. The median age is 45  years (range 1 year-100 years old; IQR 38-53 years old) with the majority of cases aged between 16-75 years. 

Individuals with higher risk for severe disease and death include those with some underlying medical conditions such as hypertension, diabetes, cardiovascular disease, chronic respiratory disease and cancer. Our sample consists of 720(4.6%) patients with chronic diseases out of which 302(41.94%) are Female and 418(58.06%) are Male. Additionally, extracting each chronic disease from 149 observations(excluding missing values) and plotting its frequency showed us that people with hypertension(34.7%)and diabetes(24.3%) are more vulnerable to COVID-19.

Since people with chronic disease are likely to face an increased risk of developing severe symptoms and eventually die, we try to find the chances of their recovery and also compare it with those who don't. The recovery:death ratio for patients with chronic disease is 17:49 and for the others is 53:8. This indeed proves that people having chronic disease, when infected by COVID19 have very low chances of recovery. 

Based on 1644 confirmed cases (excluding observations with missing values for Symptoms)  collected until March 2020, typical signs and symptoms include: Fever (32.65%), Dry Cough(18.38%), sore throat,(3.76%) pneumonia (3.5%), fatigue(2.5%), malaise(2.5%), rhinorrhea(2.3%), headache(2.23%), myalgias(2.22%), shortness of breath(1.9%), sputum(1.5%) etc. Focusing on Wuhan City, we plot a pie chart to see the initially seen symptoms. 

Fever was seen in the majority of the cases (44.4%) on its own as well as with other symptoms like cough(28.1%), weakness(2.96%), sore throat(2.96%) and fatigue(2.96%).The outbreak soon spread from China to other parts of the world. We use the geographical locations of the patients  provided in our dataset to find the places that were affected or not affected from COVID-19. The map reveals that the virus was spread from Chinese city of Wuhan to more than 180 countries and territories affecting every continent except Antarctica. In addition to chronic disease, age also influences  the level of risk for disease and death.People aged more than 60 are at a higher risk than those below 60 can be concluded with the help of the statistical hypothesis testing such as t-test. Creating a separate data frame of those who recovered (372 observations) , we create a boxplot that shows the median age as 45 (IQR 30-53) and also the average age to be 43.29.  We then create another data frame of those who died and created a boxplot. The median in this plot is 67 (IQR 55-79). 

According to WHO, the recovery time tends to be about two weeks for those with mild symptoms and about 3-6 weeks for those with severe or critical disease. However, these seem to be only rough guidelines as studies have already shown a number of exceptions. We plot the timeline for recovery for some patients and see variations in the number of days taken for recovery. With this, we can conclude that a window of 2-4 weeks can be considered as recovery time. Similarly, we plot the timeline for death for some patients. In this case, most of the patients died within 3 weeks whereas the majority of the patients older than 70 took less than 2 weeks.These are analysis that we have made on the COVID-19 patient medical data.


### **Time Series Prediction**
We need to know that no prediction is certain as long as once in a while the past repeats. There are different factors that come into play while doing the prediction such as psychological which emphasizes more on how people distinguish and react in a dangerous situation, availability of data and the variable used. Assuming that the information used is reliable which in future will follow the past trends of the disease, our forecasts say that there will be an increment within the confirmed COVID-19 cases ( deaths and recovered ) with a slight instability. <br>
We can see that in Germany the restrictions have taken important steps towards the containment of the virus. This has led to fewer deaths and confirmed cases, as for example in the US.  
It is interesting to note that the strict Spanish restrictions on the virus have made only a small difference to the less stringent restrictions in Germany (confirmed cases). <br>

# **Twitter Analysis**

The objective of this Twitter sentiment analysis was to identify the emotions and sentiment direction of the public during the corona virus outbreak. Based on the plots, it is revealed that people had more positive sentiment towards the situation rather than the negative feelings. Furthermore , it is identified that anticipation and trust are the most 
expressed emotions during the pandemic. When analyzed the frequent words used to express sentiments, it is found that "ugh", "die", "miss" , "sue", "worse" are the words used frequently to express negative sentiment while, "good", "trump", "love", "hug", "wow" are the words used for positive sentiment.
When analyzed the word cloud for German tweets, we could identify some of the words like "schon", "deutschland", "pandamie", "lockdown", "youtube" etc have been used frequently in the tweets.



# **References** 